{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you will encounter a problem that can't be solved analytically. Part of this course is helping you identify problems that you can solve analytically and giving you the tools to do it. But what if you run into a problem that you can't solve using analytical techniques?\n",
    "\n",
    "Sometimes you will encounter problems of this nature. No, not sometimes; virtually all problems of interest require a numerical solution. This isn't just a simple task of finding the right software package either; solving numerical problems requires knowledge of the type of problem at hand and what techniques are appropriate. We could spend an entire quarter or more discussing numerical methods, but we are primarily going to limit ourselves to finite difference methods here.\n",
    "\n",
    "Mathematical problems can be divided into four types:\n",
    "\n",
    "| **Category** | Algebra | Analysis |\n",
    "| ------------ | ------- | -------- |\n",
    "| Linear       | Computable | Not computable |\n",
    "| Non-linear   | Not computable | Not computable |\n",
    "\n",
    "In algebra, all constructs are *finite*, but in analysis, *transfinite* constructs e.g. limits are allowed. But computers can only deal with finite problems. Finding ways around this is part of the art of numerical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts from analysis\n",
    "\n",
    "The central idea of analysis that distinguish it from algebra are **limits** and **convergence**. The two most important concepts are **derivatives** and **integrals**. The derivative is defined as:\n",
    "\n",
    "$$\\frac{df}{dx} = \\lim_{h \\rightarrow \\infty}{\\frac{f(x + h) - f(x)}{h}} $$\n",
    "\n",
    "Later, we will look at several different expressions for approximating the derivative. When it comes to computation, we aren't just interested in whether the derivative converges. We also want **fast convergence**, so we look for tradeoffs between accuracy and computational effort.\n",
    "\n",
    "Let's look at one approximation of the derivative:\n",
    "\n",
    "$$\\frac{df}{dx} \\approx \\frac{f(x + \\Delta x) - f(x)}{\\Delta x}$$\n",
    "\n",
    "How does this approximation perform? Let's test it out using $f(x) = e^x$. In this case, we know that $f'(1) = e$, so the relative error can be expressed as:\n",
    "\n",
    "$$r = \\frac{f(1 + \\Delta x) - f(1)}{e \\Delta x} - 1 = \\frac{e^{\\Delta x} - 1}{\\Delta x} - 1 = \\frac{\\Delta x}{2} + O(\\Delta x^2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy.ma as ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delx = np.logspace(-1, -15, 1000)\n",
    "r = np.abs((np.exp(delx) - 1)/delx - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.loglog(delx, r)\n",
    "ax.set_xlabel(r'$\\Delta x$', fontsize=20)\n",
    "ax.set_ylabel('Relative Error', fontsize=20)\n",
    "#ax.set_ylim([10**-12, 10**1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when $\\Delta x < 10^{-8}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called a first-order approximation, because the error scales with $\\Delta x^1$. First-order approximations are a good start, but rarely adequate in most scientific computing. Is it possible to improve the accuracy? Yes. Let's try a second-order approximation:\n",
    "\n",
    "$$\\frac{df}{dx} \\approx \\frac{f(1 + \\Delta x) - f(1 - \\Delta x)}{2 \\Delta x} $$\n",
    "\n",
    "In this case, the relative error is:\n",
    "\n",
    "$$r = \\frac{f(1 + \\Delta x) - f(1 - \\Delta x)}{2e \\Delta x} - 1 = \\frac{e^{\\Delta x} - e^{- \\Delta x}}{2 \\Delta x}- 1 = \\frac{\\Delta x^2}{6} + O(\\Delta x^4)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.abs((np.exp(delx) - np.exp(-delx))/(2*delx) - 1)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.loglog(delx, r)\n",
    "ax.set_xlabel(r'$\\Delta x$', fontsize=20)\n",
    "ax.set_ylabel('Relative Error', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that a higher order approxmiation will achieve a much higher accuracy at a relatively large $\\Delta x$. \n",
    "\n",
    "These problems demonstrate that numerical methods can't be used blindly. We need to be able to estimate and bound the error present in our approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts from algebra\n",
    "\n",
    "Computation relies on two concepts from linear algebra, **vectors** and **matrices**. We try to express a problem as:\n",
    "\n",
    "$$Ax = b $$\n",
    "\n",
    "We can solve this problem using the inverse:\n",
    "\n",
    "$$x = A^{-1}b $$\n",
    "\n",
    "In practice, computational techniques don't actually compute the inverse. Instead they use **Gaussian elimination**, which is less computationally expensive. This works by reducing our original system to:\n",
    "\n",
    "$$LUx = b $$\n",
    "\n",
    "where L and U are lower and upper triangular matrices. Solving triangular systems is a lot easier in general.\n",
    "\n",
    "In cases of very large systems, say millions of unknowns, matrix factorization becomes prohibitively computationally expensive. In this case, we resort to **iterative methods** that sacrifice obtaining an exact solution (to our roundoff error) in favor of less computational power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts of numerical methods\n",
    "\n",
    "All computational methods rely on four basic principles:\n",
    "\n",
    "1. Discretization\n",
    "2. Linear algebra\n",
    "3. Iteration\n",
    "4. Linearization\n",
    "\n",
    "**Discretization**\n",
    "\n",
    "Discretization allows us to reduce the amount of information in a problem to a finite set (we convert analysis problems to algebra problems). Thus, $f(x)$, becomes $f(x_k)$. To see how this can be used to solve differential equations, take the example:\n",
    "\n",
    "$$\\frac{du}{dt} = \\alpha u, u(0) = u_0 $$\n",
    "\n",
    "When we express this in a discretized format, we call this the **finite difference approximation**:\n",
    "\n",
    "$$\\frac{U_{k+1} - U_{k}}{\\Delta t} = \\alpha U_k $$\n",
    "\n",
    "This can be written as a linear system of equations:\n",
    "\n",
    "$$\\begin{bmatrix} \n",
    "1 & 0 & ... & 0 \\\\\n",
    "-1 - \\alpha \\Delta t & 1 & & \\\\\n",
    " 0 & -1 - \\alpha \\Delta t &  & \\\\\n",
    " 0  & ... &  -1 - \\alpha \\Delta t & 1\n",
    "\\end{bmatrix} \\begin{bmatrix} \n",
    "U_0 \\\\\n",
    "U_1 \\\\\n",
    "... \\\\\n",
    "U_N\n",
    "\\end{bmatrix} = \\begin{bmatrix} \n",
    "u_0 \\\\\n",
    "0 \\\\\n",
    "... \\\\\n",
    "0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "As the matrix is lower triangular, this can be readily solved using forward substitution using the formula $U_{k+1} = (1 + \\alpha \\Delta t)U_k$:\n",
    "\n",
    "$$u(k \\Delta t) \\approx U_k = (1 + \\alpha \\Delta t)^k u_0$$\n",
    "\n",
    "Note that because we used a first-order approximation, the method is first-order convergent. If we want 10x accuracy, we need a 10x shorter time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 10\n",
      "Mean Squared Error: 6.558591\n",
      "Run Time (s): 0.162254\n",
      "\n",
      "N: 100\n",
      "Mean Squared Error: 0.093772\n",
      "Run Time (s): 0.0\n",
      "\n",
      "N: 1000\n",
      "Mean Squared Error: 0.000978\n",
      "Run Time (s): 0.134462\n",
      "\n",
      "N: 10000\n",
      "Mean Squared Error: 1e-05\n",
      "Run Time (s): 54.128626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set FD parameters\n",
    "\n",
    "N = 101\n",
    "for N in [11,101,1001,10001]:\n",
    "    t1 = datetime.datetime.now()\n",
    "    dt = 1/(N-1)\n",
    "    alpha = 3\n",
    "    t = np.linspace(0, 1, N)\n",
    "    # Construct A matrix\n",
    "    d1 = np.ones(N)\n",
    "    d2 = (-1-alpha*dt)*np.ones(N-1)\n",
    "    A = np.diag(d1, k=0) + np.diag(d2, k=-1)\n",
    "    # Construct b vector\n",
    "    b = np.zeros(N)\n",
    "    b[0] = 1\n",
    "    # Solve using matrix math\n",
    "    Ainv = np.linalg.inv(A)\n",
    "    U = np.matmul(Ainv,b)\n",
    "    Ua = np.exp(alpha*t)\n",
    "    r = metrics.mean_squared_error(Ua,U)\n",
    "    t2 = datetime.datetime.now()\n",
    "    trun = t2 - t1\n",
    "    trun = trun.total_seconds()\n",
    "    \n",
    "    print('N: {}'.format(N-1))\n",
    "    print('Mean Squared Error: {}'.format(np.round(r, 6)))\n",
    "    print('Run Time (s): {}\\n'.format(np.round(trun, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(t, U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 10\n",
      "Mean Squared Error: 6.558591\n",
      "Run Time (s): 0.0\n",
      "\n",
      "N: 100\n",
      "Mean Squared Error: 0.093772\n",
      "Run Time (s): 0.0\n",
      "\n",
      "N: 1000\n",
      "Mean Squared Error: 0.000978\n",
      "Run Time (s): 0.002045\n",
      "\n",
      "N: 10000\n",
      "Mean Squared Error: 1e-05\n",
      "Run Time (s): 0.028337\n",
      "\n",
      "N: 100000\n",
      "Mean Squared Error: 0.0\n",
      "Run Time (s): 0.213384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set FD parameters\n",
    "\n",
    "#N = 101\n",
    "for N in [11,101,1001,10001,100001]:\n",
    "    t1 = datetime.datetime.now()\n",
    "    dt = 1/(N-1)\n",
    "    alpha = 3\n",
    "    t = np.linspace(0, 1, N)\n",
    "    U = 0*t\n",
    "    U[0] = 1\n",
    "\n",
    "    for i, ti in enumerate(t):\n",
    "        if ti > 0:\n",
    "            U[i] = U[0]*(1 + alpha*dt)**i\n",
    "\n",
    "    t2 = datetime.datetime.now()\n",
    "    trun = t2 - t1\n",
    "    trun = trun.total_seconds()\n",
    "    Ua = np.exp(alpha*t)\n",
    "    r = metrics.mean_squared_error(Ua,U)\n",
    "\n",
    "\n",
    "    print('N: {}'.format(N-1))\n",
    "    print('Mean Squared Error: {}'.format(np.round(r, 6)))\n",
    "    print('Run Time (s): {}\\n'.format(np.round(trun, 12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Value Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **first-order initial value problem** takes the form. Most software packages (like Python) are designed to solve first-order problems:\n",
    "\n",
    "$$\\frac{dy}{dt} = f(t, y) $$\n",
    "\n",
    "A simple example of an IVP is the second-order equation describing the **motion of a pendulum**:\n",
    "\n",
    "$$\\frac{d^2 \\phi}{dt^2} + \\frac{g}{L} \\sin{\\phi} = 0, \\phi(0) = \\phi_0, \\frac{\\phi}{dt}(0) = \\omega_0 $$\n",
    "\n",
    "Since this is a second-order problem, if we want to solve it using standard solvers, we need to use be able to express it as a system of first-order equations:\n",
    "\n",
    "$$\\frac{d \\phi}{dt} = \\omega $$\n",
    "$$\\frac{d \\omega}{dt} = -\\frac{g}{L} \\sin{\\phi} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boundary Value Problems\n",
    "\n",
    "Take, for example, the second-order problem:\n",
    "\n",
    "$$-\\frac{d^2u}{dx^2} = f(x) $$\n",
    "\n",
    "**Dirichlet boundary conditions** take the form $u(0)=u_L$ and $u(1) = u_R$. \n",
    "\n",
    "**Neumann boundary conditions** take the form $u'(0) = u_L'$. Neumann conditions can be imposed at one end, but in order to completely solve, you must have at least one Dirichlet condition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
